{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-13T06:42:57.129028Z",
     "start_time": "2024-11-13T05:56:10.990386Z"
    }
   },
   "source": [
    "#Task 12\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "# Load the interactions dataset with user ratings\n",
    "interactions_df = pd.read_csv('RAW_interactions_filtered.csv')\n",
    "\n",
    "# Split data into training and test sets (80% training, 20% testing for each user)\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for user_id, user_ratings in interactions_df.groupby('user_id'):\n",
    "    if len(user_ratings) < 5:  # threshold that makes sense for data\n",
    "        continue  # Skip or handle users with very few ratings\n",
    "    # Split ratings for each user\n",
    "    user_train, user_test = train_test_split(user_ratings, test_size=0.2, random_state=42)\n",
    "    train_data.append(user_train)\n",
    "    test_data.append(user_test)\n",
    "\n",
    "# Concatenate user train and test sets into single DataFrames\n",
    "train_df = pd.concat(train_data)\n",
    "test_df = pd.concat(test_data)\n",
    "\n",
    "def predict_rating_c(user_id, recipe_id, train_ratings, similarity_matrix):\n",
    "    # Filter the user's ratings from the training set\n",
    "    user_train_ratings = train_ratings[train_ratings['user_id'] == user_id]\n",
    "    rated_recipes = user_train_ratings['recipe_id'].values\n",
    "    ratings = user_train_ratings['rating'].values\n",
    "    \n",
    "    # Find similarity scores between target recipe and all recipes user rated in training set\n",
    "    similarities = similarity_matrix.loc[recipe_id, rated_recipes].values\n",
    "    \n",
    "    # Weighted average of the ratings based on similarity\n",
    "    if similarities.sum() > 0:\n",
    "        predicted_rating = np.dot(similarities, ratings) / similarities.sum()\n",
    "    else:\n",
    "        predicted_rating = np.mean(ratings)  # Default to average rating if no similarities\n",
    "\n",
    "    return predicted_rating\n",
    "\n",
    "# Load the dataset\n",
    "recipes_df = pd.read_csv('RAW_recipes_filtered.csv')\n",
    "\n",
    "# Preprocess the ingredients column\n",
    "# Join the list of ingredients into a single string for each recipe\n",
    "recipes_df['ingredients_text'] = recipes_df['ingredients'].apply(lambda x: ' '.join(eval(x)) if pd.notna(x) else '')\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer_in = TfidfVectorizer()\n",
    "tfidf_matrix_in= tfidf_vectorizer_in.fit_transform(recipes_df['ingredients_text'])\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim_matrix_in = cosine_similarity(tfidf_matrix_in)\n",
    "\n",
    "# Convert the similarity matrix to a DataFrame for easy lookup\n",
    "similarity_df_in = pd.DataFrame(cosine_sim_matrix_in, index=recipes_df['id'], columns=recipes_df['id'])\n",
    "\n",
    "# Load the dataset\n",
    "recipes_df_1 = pd.read_csv('RAW_recipes_filtered.csv')\n",
    "\n",
    "# Preprocess the descriptions by filling NaNs with an empty string\n",
    "recipes_df_1['description'] = recipes_df_1['description'].fillna('')\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer_re = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_re= tfidf_vectorizer_re.fit_transform(recipes_df_1['description'])\n",
    "\n",
    "# Compute the cosine similarity matrix based on descriptions\n",
    "cosine_sim_matrix_re = cosine_similarity(tfidf_matrix_re)\n",
    "\n",
    "# Convert the similarity matrix to a DataFrame for easy lookup\n",
    "similarity_df_re = pd.DataFrame(cosine_sim_matrix_re, index=recipes_df_1['id'], columns=recipes_df_1['id'])\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "recipes_df_tag= pd.read_csv('RAW_recipes_filtered.csv')\n",
    "\n",
    "# Preprocess the tags column\n",
    "# Join the list of tags into a single string for each recipe\n",
    "recipes_df_tag['tags_text'] = recipes_df_tag['tags'].apply(lambda x: ' '.join(eval(x)) if pd.notna(x) else '')\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer_tag = TfidfVectorizer()\n",
    "tfidf_matrix_tag= tfidf_vectorizer_tag.fit_transform(recipes_df_tag['tags_text'])\n",
    "\n",
    "# Compute the cosine similarity matrix based on tags\n",
    "cosine_sim_matrix_tag = cosine_similarity(tfidf_matrix_tag)\n",
    "\n",
    "# Convert the similarity matrix to a DataFrame for easy lookup\n",
    "similarity_df_tag = pd.DataFrame(cosine_sim_matrix_tag, index=recipes_df_tag['id'], columns=recipes_df_tag['id'])\n",
    "# Save the dataset similarity_df\n",
    "\n",
    "\n",
    "ingredient_similarity_df = similarity_df_in\n",
    "description_similarity_df = similarity_df_tag\n",
    "review_similarity_df = similarity_df_re\n",
    "\n",
    "def calculate_user_similarity(train_df):\n",
    "    # Pivot the training data to get a user-item rating matrix\n",
    "    user_item_matrix = train_df.pivot(index='user_id', columns='recipe_id', values='rating')\n",
    "    user_means = user_item_matrix.mean(axis=1)\n",
    "\n",
    "    # Subtract the user's average rating from each rating to center data\n",
    "    user_item_matrix_centered = user_item_matrix.sub(user_means, axis=0)\n",
    "\n",
    "    # Calculate the Pearson correlation coefficient matrix between users\n",
    "    user_similarity = user_item_matrix_centered.T.corr(method='pearson').fillna(0)\n",
    "    return user_similarity\n",
    "\n",
    "user_similarity = calculate_user_similarity(train_df)\n",
    "\n",
    "\n",
    "def predict_rating(user_id, recipe_id, train_df, user_similarity, k=5):\n",
    "    # Filter the user's ratings from the training set\n",
    "    user_train_ratings = train_df[train_df['recipe_id'] == recipe_id]\n",
    "    if user_train_ratings.empty:\n",
    "        return np.nan  # If no ratings, we can't predict\n",
    "\n",
    "    # Find similar users who have rated this recipe\n",
    "    similarities = user_similarity[user_id]\n",
    "    ratings_by_similar_users = user_train_ratings.set_index('user_id')['rating']\n",
    "\n",
    "    # Keep only the top k most similar users who have rated the recipe\n",
    "    similar_users = similarities[ratings_by_similar_users.index].nlargest(k)\n",
    "    if similar_users.sum() == 0:\n",
    "        return np.nan  # If no similar users, we can't predict\n",
    "\n",
    "    # Calculate weighted average rating\n",
    "    weighted_ratings = ratings_by_similar_users.loc[similar_users.index] * similar_users\n",
    "    predicted_rating = weighted_ratings.sum() / similar_users.sum()\n",
    "    return predicted_rating\n",
    "\n",
    "\n",
    "# Define a function for the content-based prediction as done in Task 10\n",
    "def content_based_predict(user_id, recipe_id, train_ratings, ingredient_sim, desc_sim, review_sim, alpha1=0.4, alpha2=0.3, alpha3=0.3):\n",
    "    # Predict using a weighted combination of ingredient, description, and review-based similarities\n",
    "    content_score = (alpha1 * predict_rating_c(user_id, recipe_id, train_ratings, ingredient_sim) +\n",
    "                     alpha2 * predict_rating_c(user_id, recipe_id, train_ratings, desc_sim) +\n",
    "                     alpha3 * predict_rating_c(user_id, recipe_id, train_ratings, review_sim))\n",
    "    return content_score\n",
    "\n",
    "# Define a function for the collaborative filtering prediction as done in Task 11\n",
    "def collaborative_filtering_predict(user_id, recipe_id, train_ratings, user_similarity, k=5):\n",
    "    return predict_rating(user_id, recipe_id, train_ratings, user_similarity, k)\n",
    "\n",
    "# Define the hybrid prediction function\n",
    "def hybrid_predict(user_id, recipe_id, train_ratings, ingredient_sim, desc_sim, review_sim, user_similarity, alpha=0.5):\n",
    "    # Content-based prediction\n",
    "    content_score = content_based_predict(user_id, recipe_id, train_ratings, ingredient_sim, desc_sim, review_sim)\n",
    "    \n",
    "    # Collaborative filtering prediction\n",
    "    collaborative_score = collaborative_filtering_predict(user_id, recipe_id, train_ratings, user_similarity)\n",
    "    \n",
    "    # Combine predictions\n",
    "    if np.isnan(content_score) and not np.isnan(collaborative_score):\n",
    "        return collaborative_score\n",
    "    elif np.isnan(collaborative_score) and not np.isnan(content_score):\n",
    "        return content_score\n",
    "    elif np.isnan(content_score) and np.isnan(collaborative_score):\n",
    "        return np.nan  # No prediction if both are NaN\n",
    "\n",
    "    # Weighted average of content and collaborative predictions\n",
    "    hybrid_score = alpha * content_score + (1 - alpha) * collaborative_score\n",
    "    return hybrid_score\n",
    "\n",
    "\n",
    "# Evaluate the hybrid model\n",
    "def evaluate_hybrid_model(test_df, train_df, ingredient_sim, desc_sim, review_sim, user_similarity, alpha=0.5):\n",
    "    true_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        recipe_id = row['recipe_id']\n",
    "        true_rating = row['rating']\n",
    "        \n",
    "        # Predict rating based on the hybrid model\n",
    "        predicted_rating = hybrid_predict(user_id, recipe_id, train_df, ingredient_sim, desc_sim, review_sim, user_similarity, alpha)\n",
    "        \n",
    "        if not np.isnan(predicted_rating):\n",
    "            # Store true and predicted ratings\n",
    "            true_ratings.append(true_rating)\n",
    "            predicted_ratings.append(predicted_rating)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mae = mean_absolute_error(true_ratings, predicted_ratings).round(2)\n",
    "    rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings)).round(2)\n",
    "    return mae, rmse\n",
    "\n",
    "# Run evaluation for the hybrid model with a chosen alpha\n",
    "alpha = 0.5  # Adjust this value to balance content and collaborative filtering\n",
    "mae, rmse = evaluate_hybrid_model(test_df, train_df, ingredient_similarity_df, description_similarity_df, review_similarity_df, user_similarity, alpha=alpha)\n",
    "print(f\"Hybrid Recommendation Model - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Create a DataFrame to save the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model Based': ['Hybrid'],\n",
    "    'MAE': [mae],\n",
    "    'RMSE': [rmse]\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('/evaluation_results_12.csv', index=False)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Recommendation Model - MAE: 0.5400, RMSE: 1.0400\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5ce5dd9667e6bc02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
